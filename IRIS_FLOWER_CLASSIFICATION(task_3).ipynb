{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVn5mSfvdvIlIPhXvD41yL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MEDA-NAVADURGA/CODSOFT/blob/main/IRIS_FLOWER_CLASSIFICATION(task_3).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IRIS FLOWER CLASSIFICATION:-\n",
        "============================================\n",
        "problem statement:-\n",
        "--------------------\n",
        "*The Iris flower dataset consists of three species: setosa, versicolor,\n",
        "and virginica. These species can be distinguished based on their\n",
        "measurements. Now, imagine that you have the measurements\n",
        "of Iris flowers categorized by their respective species.\n",
        " Your\n",
        "objective is to train a machine learning model that can learn from\n",
        "these measurements and accurately classify the Iris flowers into\n",
        "their respective species.\n",
        "\n",
        "Use the Iris dataset to develop a model that can classify iris\n",
        "flowers into different species based on their sepal and petal\n",
        "measurements. This dataset is widely used for introductory\n",
        "classification tasks.\n",
        "processing steps:-\n",
        "========================\n",
        "1) importing required libraries.\n",
        "\n",
        "2) Exploring the Credit Card Fraud dataset.\n",
        "\n",
        "3) Data preprocessing."
      ],
      "metadata": {
        "id": "6iUruUqv7suT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "print(\"installation successfully completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmLKKe1GAe6Q",
        "outputId": "93850309-0757-4033-ff21-c2fde79b2e2f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "installation successfully completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SVDPCA:\n",
        "    def __init__(self, n_components=None):\n",
        "        self.n_components = n_components\n",
        "\n",
        "    @staticmethod\n",
        "    def svd_flip_vector(U):\n",
        "        max_abs_cols_U = np.argmax(np.abs(U), axis=0)\n",
        "        # extract the signs of the max absolute values\n",
        "        signs_U = np.sign(U[max_abs_cols_U, range(U.shape[1])])\n",
        "\n",
        "        return U * signs_U\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        n_samples, n_features = X.shape\n",
        "        X_centered = X - X.mean(axis=0)\n",
        "\n",
        "        if self.n_components is None:\n",
        "            self.n_components = min(n_samples, n_features)\n",
        "\n",
        "        U, S, Vt = np.linalg.svd(X_centered)\n",
        "        # flip the eigenvector sign to enforce deterministic output\n",
        "        U_flipped = self.svd_flip_vector(U)\n",
        "\n",
        "        self.explained_variance = (S[:self.n_components] ** 2) / (n_samples - 1)\n",
        "        self.explained_variance_ratio = self.explained_variance / np.sum(self.explained_variance)\n",
        "\n",
        "        # X_new = X * V = U * S * Vt * V = U * S\n",
        "        X_transformed = U_flipped[:, : self.n_components] * S[: self.n_components]\n",
        "\n",
        "        return X_transformed"
      ],
      "metadata": {
        "id": "FnNrjt3AAvcv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Uploading a dataset\n",
        "df_path = \"/content/IRIS.csv\"\n",
        "iris_df = pd.read_csv(df_path)\n",
        "X, y = iris_df.iloc[:, :-1], iris_df.iloc[:, -1]\n",
        "y = pd.Series(LabelEncoder().fit_transform(y))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "print(iris_df)"
      ],
      "metadata": {
        "id": "nOEm-3kEBJ-Q",
        "outputId": "33cd4a16-b46a-4d1e-d062-8b4c84c4eea8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     sepal_length  sepal_width  petal_length  petal_width         species\n",
            "0             5.1          3.5           1.4          0.2     Iris-setosa\n",
            "1             4.9          3.0           1.4          0.2     Iris-setosa\n",
            "2             4.7          3.2           1.3          0.2     Iris-setosa\n",
            "3             4.6          3.1           1.5          0.2     Iris-setosa\n",
            "4             5.0          3.6           1.4          0.2     Iris-setosa\n",
            "..            ...          ...           ...          ...             ...\n",
            "145           6.7          3.0           5.2          2.3  Iris-virginica\n",
            "146           6.3          2.5           5.0          1.9  Iris-virginica\n",
            "147           6.5          3.0           5.2          2.0  Iris-virginica\n",
            "148           6.2          3.4           5.4          2.3  Iris-virginica\n",
            "149           5.9          3.0           5.1          1.8  Iris-virginica\n",
            "\n",
            "[150 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model training and evaluation of the obtained results:-\n",
        "============================================================\n",
        "The manual implementation showed identical scikit-learn results. As you can see, the first 2 main components explain almost 98% of the variance in the data which allows you to reduce the number of features by half without much loss of information. If the number of features were not 4 but several thousand or millions, then this would significantly reduce the training time of models without significant loss of accuracy (and sometimes with increased accuracy by reducing multicollinearity between features) what makes PCA and its alternatives an excellent addition to other algorithms."
      ],
      "metadata": {
        "id": "DKjMHNQkBjK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pca(Principal Component Analysis (PCA))\n",
        "pca = SVDPCA()\n",
        "X_transformed = pca.fit_transform(X)\n",
        "\n",
        "print('transformed data', X_transformed[:10], '', sep='\\n')\n",
        "print('explained_variance', pca.explained_variance)\n",
        "print('explained_variance_ratio', pca.explained_variance_ratio)"
      ],
      "metadata": {
        "id": "Vnj8HXdIBo8_",
        "outputId": "5774b326-6818-4fda-b299-0ae561bcde28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformed data\n",
            "[[-2.68420713e+00  3.26607315e-01 -2.15118370e-02  1.00615724e-03]\n",
            " [-2.71539062e+00 -1.69556848e-01 -2.03521425e-01  9.96024240e-02]\n",
            " [-2.88981954e+00 -1.37345610e-01  2.47092410e-02  1.93045428e-02]\n",
            " [-2.74643720e+00 -3.11124316e-01  3.76719753e-02 -7.59552741e-02]\n",
            " [-2.72859298e+00  3.33924564e-01  9.62296998e-02 -6.31287327e-02]\n",
            " [-2.27989736e+00  7.47782713e-01  1.74325619e-01 -2.71468037e-02]\n",
            " [-2.82089068e+00 -8.21045110e-02  2.64251085e-01 -5.00996251e-02]\n",
            " [-2.62648199e+00  1.70405349e-01 -1.58015103e-02 -4.62817610e-02]\n",
            " [-2.88795857e+00 -5.70798026e-01  2.73354061e-02 -2.66154143e-02]\n",
            " [-2.67384469e+00 -1.06691704e-01 -1.91533300e-01 -5.58909660e-02]]\n",
            "\n",
            "explained_variance [4.22484077 0.24224357 0.07852391 0.02368303]\n",
            "explained_variance_ratio [0.92461621 0.05301557 0.01718514 0.00518309]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA (scikit-learn)\n",
        "sk_pca = PCA()\n",
        "sk_X_transformed = sk_pca.fit_transform(X)\n",
        "\n",
        "print('sk transformed data', sk_X_transformed[:10], '', sep='\\n')\n",
        "print('sk explained_variance', sk_pca.explained_variance_)\n",
        "print('sk explained_variance_ratio_', sk_pca.explained_variance_ratio_)"
      ],
      "metadata": {
        "id": "kOHOYQjsCaNP",
        "outputId": "3c2f3782-b93a-4a84-81c5-4dd0d801b569",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sk transformed data\n",
            "[[-2.68420713e+00  3.26607315e-01 -2.15118370e-02  1.00615724e-03]\n",
            " [-2.71539062e+00 -1.69556848e-01 -2.03521425e-01  9.96024240e-02]\n",
            " [-2.88981954e+00 -1.37345610e-01  2.47092410e-02  1.93045428e-02]\n",
            " [-2.74643720e+00 -3.11124316e-01  3.76719753e-02 -7.59552741e-02]\n",
            " [-2.72859298e+00  3.33924564e-01  9.62296998e-02 -6.31287327e-02]\n",
            " [-2.27989736e+00  7.47782713e-01  1.74325619e-01 -2.71468037e-02]\n",
            " [-2.82089068e+00 -8.21045110e-02  2.64251085e-01 -5.00996251e-02]\n",
            " [-2.62648199e+00  1.70405349e-01 -1.58015103e-02 -4.62817610e-02]\n",
            " [-2.88795857e+00 -5.70798026e-01  2.73354061e-02 -2.66154143e-02]\n",
            " [-2.67384469e+00 -1.06691704e-01 -1.91533300e-01 -5.58909660e-02]]\n",
            "\n",
            "sk explained_variance [4.22484077 0.24224357 0.07852391 0.02368303]\n",
            "sk explained_variance_ratio_ [0.92461621 0.05301557 0.01718514 0.00518309]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cons:\n",
        "\n",
        "the inevitable loss of some information in the data;\n",
        "search only for linear dependence in the data (in the classic PCA);\n",
        "the lack of semantic meaning of the main components due to the difficulty of linking them with real features"
      ],
      "metadata": {
        "id": "5fDZklrEC4X_"
      }
    }
  ]
}